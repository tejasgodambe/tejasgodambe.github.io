<!DOCTYPE html>
<html>
<body>

<h1>Tejas Godambe</h1>
<h2> Bayesian inference </h2>
<p> <a href="http://www.kdnuggets.com/2016/12/bayesian-basics-explained.html"> </a> </p>
Bayesian statistics uses the mathematical rules of probability to <b> combines data with “prior information” </b> to give inferences which (if the model being used is correct) are more precise than would be obtained by either source of information alone. </p>

<p>
The essence of Bayesian statistics is the combination of information from multiple sources.  We call this data and prior information, or hierarchical modeling, or dynamic updating, or partial pooling, but in any case it’s all about putting together data to understand a larger structure. </p>

<h2> TDNN </h2>
<p>
A very concise and quick to understand explanation of TDNNs can be found in page 2, para 2 of <a href=https://clgiles.ist.psu.edu/papers/IEEE.TNN.tdnn.as.fsm.pdf"> Link </a>
</p>

<h2> Vanishing gradient problem </h2>
<p> A fantastic explanation of the vanishing gradient problem <a href="https://www.quora.com/What-is-the-vanishing-gradient-problem"> </a> </p>

</body>
</html>
